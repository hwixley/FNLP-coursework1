lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
best10_ents=[(4.84723281788351, ['t']), (5.137306855634625, ['s']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d'])]
worst10_ents=[(633.1919363477411, ['รห', 'ส', 'รห', 'ส', 'ร', 'กท', 'กษ', 'ณรห', 'ส', 'เอ', 'นด', 'ป', 'าเต', 'ง', 'ชอบอภ', 'ส', 'ทธ', 'รห', 'ส', 'รห', 'สบร', 'จาคเง', 'นภาษ', 'ภ', 'ม', 'ใจเส', 'ยเป', 'ดรห', 'ส', 'เช', 'ยร', 'สนธ', 'และพ', 'นธม', 'ตรฯ']), (650.8487178327986, ['अच', 'छ', 'आय', 'इस', 'कर', 'ड', 'क', 'क', 'क', 'ज', 'त', 'म', 'ह', 'फ', 'सद', 'बढ', 'ब', 'ओब', 'ब', 'क', 'म', 'न', 'फ', 'म', 'न', 'फ', 'म', 'र', 'पए', 'श', 'ल', 'क', 'स', 'ह', 'आ', 'ह']), (650.8487178327986, ['것', '애플', '쓰는', '있고', '쓰는', '있고', '아예', '있고', '좋은', '시대', '뭐라', '할수', '삼성', '구글과', '그리고', '취향의', '사람도', '피쳐폰', '사람도', '안쓰는', '사람도', '아닐까', '취향과', '선택의', '같은걸', '지는건', '없지만', '노키아', '않을까', '스마트폰', '시대라고', '봐야하지', '다양한게', '쓰고싶어', '강요하지말자', 'MS']), (650.8487178327986, ['อ', 'ม', 'ล', 'ม', 'ราย', 'ราย', 'อ', 'ก', 'ราย', 'ห', 'วฟาด', 'ค', 'นน', 'ก', 'นเหล', 'า', 'อาการสาห', 'ส', 'ไม', 'เจ', 'บมาก', 'ข', 'บมอเตอร', 'ไซค', 'ไม', 'ใส', 'หมวกก', 'นน', 'อค', 'จะโชคด', 'ไปอ', 'กส', 'กก', 'ท', 'หนอ', 'ER']), (668.505499317856, ['ฮา', 'ก', 'บ', 'คร', 'บ', 'พ', 'ขจร', 'เข', 'าก', 'น', 'ค', 'ดย', 'งไง', 'ผมเห', 'นเดโม', 'เหม', 'อนเด', 'ม', 'แต', 'เห', 'นว', 'าไม', 'รองร', 'บ', 'ท', 'พ', 'ให', 'ด', 'แล', 'วม', 'นเข', 'าก', 'น', 'Flash', 'iPad', 'NewYork', 'Times']), (703.8190622879708, ['ஆண', 'ட', 'கள', 'ல', 'ஆப', 'ப', 'ள', 'இன', 'ன', 'ம', 'ஐப', 'ட', 'க', 'ப', 'ப', 'ச', 'ய', 'ச', 'ல', 'ந', 'ற', 'வனத', 'த', 'ன', 'ப', 'த', 'ய', 'வ', 'ழ', 'ங', 'க', 'வ', 'ட', 'ல', 'ம', 'வ', 'ள', 'ய', 'ட']), (721.4758437730283, ['Elysium', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'playing']), (774.4461882282005, ['lt', 'ஆண', 'ட', 'ஆண', 'டவர', 'கள', 'க', 'க', 'இடம', 'இன', 'ற', 'ஒர', 'ஒர', 'க', 'ட', 'க', 'க', 'றத', 'gt', 'த', 'ன', 'பரம', 'பர', 'பரம', 'பர', 'க', 'க', 'பரம', 'பர', 'ய', 'ப', 'ர', 'ட', 'ட', 'வர', 'டம', 'வ', 'ல', 'வ', 'ய', 'ப', 'ப', 'ல']), (792.102969713258, ['ന', 'ന', 'ന', 'പ', 'ന', 'മ', 'റ', 'മ', 'ര', 'സയ', 'മ', 'യ', 'ള', 'ള', 'മ', 'ഹമ', 'മദ', 'വ', 'വ', 'ഹത', 'ത', 'വ', 'വ', 'ഹത', 'ത', 'ല', 'സ', 'ന', 'യ', 'സ', 'ഹ', 'ത', 'ത', 'സ', 'റ', 'ബ', 'ഹ', 'ദര', 'ബ', 'ദ', 'ബ', 'ല', 'യക', 'ല']), (1021.6411290190044, ['ആദ', 'യ', 'ഇന', 'ത', 'യന', 'ഇല', 'ല', 'ട', 'മ', 'ന', 'ട', 'സ', 'റ', 'റ', 'ന', 'ള', 'ള', 'ദക', 'ഷ', 'ണ', 'ഫ', 'ര', 'ക', 'കക', 'ക', 'ത', 'ര', 'യ', 'ദ', 'ര', 'വ', 'ഡ', 'പര', 'ക', 'ക', 'റ', 'റ', 'പ', 'രഖ', 'യ', 'പ', 'ച', 'ച', 'യ', 'വര', 'ജ', 'ര', 'ഹ', 'ല', 'ശ', 'ര', 'ശ', 'ന', 'ത', 'സ', 'ങ', 'ങ'])]
answer_open_question_3='The first tweets are all unit length and have a\nsingle character. We can imagine these give smallest entropies\nas they are less novel than a typical English sentence.\nThe last tweets mainly consisted of logograms from other\nlanguages. This was to be expected given these languages are\nevidently not likely to be used in an English tweet. However,\nsurprsingly the fourth last tweet consisted of English\ncharacters with "gt" repeating 38 times indicating "gt"\nmust just have a high word entropy.'
answer_open_question_4='We should remove all non-English tweets from the corpus\nas these characters/words are obviously not relevant for\ndeveloping an English NL model.\nWe can identify non-English tweets by checking if they contain \nnon-English characters, or simply just using an existing\nlanguage detection model (ie. TextBlob).'
mean=162.0560161994317
std=95.30201424838675
best10_ascci_ents=[(4.84723281788351, ['t']), (5.137306855634625, ['s']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d']), (5.894416382928467, ['d'])]
worst10_ascci_ents=[(367.62821754233147, ['a', 'a', 'across', 'and', 'apple', 'bits', 'cake', 'cake', 'came', 'caramel', 'choc', 'for', 'I', 'kind', 'looking', 'of', 'really', 'recipe', 'toffee', 'what', 'with', 'yummy']), (367.62821754233147, ['a', 'a', 'and', 'as', 'Came', 'come', 'D', 'got', 'here', 'hollaaa', 'I', 'last', 'MacBook', 'me', 'Oh', 'pro', 'semester', 'shocker', 'to', 'truly', 'who', 'Yours']), (367.62821754233147, ['a', 'a', 'abdomen', 'and', 'arm', 'chola', 'cockeyed', 'girl', 'id', 'into', 'like', 'lopsided', 'my', 'my', 'on', 'on', 'parlor', 'tattoo', 'titty', 'uh', 'walks', 'yeah']), (367.62821754233147, ['a', 'a', 'beau', 'Birdwatching', 'blonde', 'day', 'former', 'got', 'in', 'is', 'I', 've', 'NSFW', 'parter', 'Polish', 'question', 'start', 'The', 'to', 'two', 'with', 'your']), (367.62821754233147, ['a', 'a', 'changed', 'costume', 'dream', 'Either', 'feel', 'goth', 'halloween', 'I', 'I', 'I', 've', 'million', 'my', 'or', 'overwhelmed', 'party', 'times', 'victorian', 'want', 'wedding']), (367.62821754233147, ['a', 'a', 'A', 'My', 'and', 'family', 'father', 'guys', 'is', 'my', 'of', 'oldest', 'ones', 'only', 'Q', 'Are', 'singer', 'sister', 'tha', 'the', 'you', 'your']), (367.62821754233147, ['a', 'a', 'from', 'home', 'in', 'it', 'journey', 'literally', 'major', 'neck', 'nerve', 'or', 'pain', 'Quite', 'something', 'spare', 'the', 'the', 'to', 'Trapped', 'tube', 'Working']), (367.62821754233147, ['a', 'a', 'and', 'are', 'as', 'been', 'by', 'chosen', 'company', 'confidence', 'consumer', 'for', 'have', 'highly', 'model', 'proud', 'respected', 'role', 'such', 'to', 'trust', 'We']), (367.62821754233147, ['a', 'a', 'been', 'book', 'bought', 'but', 'buy', 'going', 'had', 'homeopathy', 'I', 'I', 'instead', 'next', 'notepad', 'on', 'one', 'RT', 'that', 'to', 'to', 'was']), (367.62821754233147, ['a', 'a', 'actress', 'after', 'airport', 'at', 'Bollywood', 'by', 'Delhi', 'jumping', 'of', 'off', 'off', 'Preity', 'pulled', 'RT', 'stunt', 'the', 'the', 'toilet', 'wall', 'Zinta'])]
best10_non_eng_ents=[(257.3595092475111, ['cancelarelecciones', 'liberar', 'mujeres', 'N', 'o', 'p', 'aborto', 'pls', 'políticos', 'presas', 'RT', 'RT', 'RT', 'Solicitamos', 'Veracruz']), (257.4065402450734, ['acho', 'de', 'de', 'eu', 'férias', 'incrível', 'novo', 'ns', 'pareça', 'por', 'preciso', 'q', 'q', 'x']), (257.4087262629656, ['b', 'bai', 'cuz', 'da', 'day', 'do', 'Friday', 'gotta', 'his', 'like', 'sumthin', 'til', 'wait', 'we', 'weekend']), (257.4088375262885, ['b', 'cae', 'con', 'de', 'enmedio', 'estamos', 'examenes', 'ir', 'jajjajaja', 'me', 'no', 'o', 'pensando', 'si', 'si', 'tia']), (257.40903855599663, ['b', 'bin', 'bothered', 'cant', 'cuz', 'empty', 'goin', 'in', 'kat', 'kit', 'pocket', 'Put', 'th', 'u', 'ur', 'wrappers']), (257.4288622383092, ['an', 'and', 'aversion', 'bills', 'change', 'hate', 'have', 'i', 'i', 'intense', 'paper', 'spare', 'that', 'to', 'ughhh']), (257.4288622383092, ['are', 'asked', 'books', 'what', 'have', 'him', 'and', 'i', 'i', 'into', 'lots', 'meant', 'of', 'shoulda', 'you']), (257.4288622383092, ['again', 'baby', 'be', 'do', 'feelings', 'have', 'i', 'i', 'S', 'seeing', 'that', 'the', 'today', 'why', 'wont']), (257.4288622383092, ['asks', 'booo', 'dont', 'doode', 'goodness', 'gotttaa', 'graciouuuus', 'haha', 'i', 'i', 'if', 'more', 'ONE', 'sweaaar', 'why']), (257.4288622383092, ['and', 'and', 'at', 'boom', 'got', 'have', 'homeeee', 'i', 'i', 'of', 'out', 'pink', 'taxi', 'the', 'white'])]
worst10_non_eng_ents=[(367.62821754233147, ['a', 'a', 'across', 'and', 'apple', 'bits', 'cake', 'cake', 'came', 'caramel', 'choc', 'for', 'I', 'kind', 'looking', 'of', 'really', 'recipe', 'toffee', 'what', 'with', 'yummy']), (367.62821754233147, ['a', 'a', 'and', 'as', 'Came', 'come', 'D', 'got', 'here', 'hollaaa', 'I', 'last', 'MacBook', 'me', 'Oh', 'pro', 'semester', 'shocker', 'to', 'truly', 'who', 'Yours']), (367.62821754233147, ['a', 'a', 'abdomen', 'and', 'arm', 'chola', 'cockeyed', 'girl', 'id', 'into', 'like', 'lopsided', 'my', 'my', 'on', 'on', 'parlor', 'tattoo', 'titty', 'uh', 'walks', 'yeah']), (367.62821754233147, ['a', 'a', 'beau', 'Birdwatching', 'blonde', 'day', 'former', 'got', 'in', 'is', 'I', 've', 'NSFW', 'parter', 'Polish', 'question', 'start', 'The', 'to', 'two', 'with', 'your']), (367.62821754233147, ['a', 'a', 'changed', 'costume', 'dream', 'Either', 'feel', 'goth', 'halloween', 'I', 'I', 'I', 've', 'million', 'my', 'or', 'overwhelmed', 'party', 'times', 'victorian', 'want', 'wedding']), (367.62821754233147, ['a', 'a', 'A', 'My', 'and', 'family', 'father', 'guys', 'is', 'my', 'of', 'oldest', 'ones', 'only', 'Q', 'Are', 'singer', 'sister', 'tha', 'the', 'you', 'your']), (367.62821754233147, ['a', 'a', 'from', 'home', 'in', 'it', 'journey', 'literally', 'major', 'neck', 'nerve', 'or', 'pain', 'Quite', 'something', 'spare', 'the', 'the', 'to', 'Trapped', 'tube', 'Working']), (367.62821754233147, ['a', 'a', 'and', 'are', 'as', 'been', 'by', 'chosen', 'company', 'confidence', 'consumer', 'for', 'have', 'highly', 'model', 'proud', 'respected', 'role', 'such', 'to', 'trust', 'We']), (367.62821754233147, ['a', 'a', 'been', 'book', 'bought', 'but', 'buy', 'going', 'had', 'homeopathy', 'I', 'I', 'instead', 'next', 'notepad', 'on', 'one', 'RT', 'that', 'to', 'to', 'was']), (367.62821754233147, ['a', 'a', 'actress', 'after', 'airport', 'at', 'Bollywood', 'by', 'Delhi', 'jumping', 'of', 'off', 'off', 'Preity', 'pulled', 'RT', 'stunt', 'the', 'the', 'toilet', 'wall', 'Zinta'])]
answer_open_question_6='Problems:\n1. The magnitude of words in English implies the majority of these\nentropies will be exponentially small.\n2. Which words can be classified as "proper English"\n3. '
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[7.645645775585578e-05, 7.437719469104356e-05, 0.00012304032772425798, 7.39304665711192e-05, 7.394179345097604e-05, 7.572782710921218e-05, 7.589415018154868e-05]
naive_bayes_posterior=[{'V': 0.4781844850101543, 'N': 0.5218155149898457}, {'V': 0.47750420743737804, 'N': 0.522495792562622}, {'V': 0.5210462808112636, 'N': 0.47895371918873647}, {'V': 0.5210462808112636, 'N': 0.47895371918873647}, {'V': 0.49255185071781066, 'N': 0.5074481492821894}]
naive_bayes_classify=['N', 'N']
naive_bayes_acc=0.6065857885615251
answer_open_question_8='The best accuracy was achieved using a sequence of words. Indicating that\nthis model is most useful when passed a sequence of words.\n\nMy NB accuracy is worse than all LR scores in Table 1. I believe this\ndifference can mainly be attributed to the NB independence assumption\nas this infers probability distributions about features that are likely\nnot true. Thus a model that does not assume any distribution would be\nmore useful.'
lr_predictions='VVVVVVVVVNVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNVVVNNVNNNVNNVVVNNVVVNNNVVNVNVNVNNNVVVNVNNVVVVNVNNVNVVVVNVNNVNNNNVVVNVVNVVNVVNVNNVNVNNNVVNNNNVNNNNVNNVNVVNNNNVNNNNNNNNNNVVNNVVVVVVVNVNVNNVNVVVVNVVVVVVNNNNNVVNVNNNNNNVNVVVNNNVNNNNVNNNVNNVNNVNNVNNNNVVNNNNVNVNVNNVVVNNNNNVVNNNVNVVVVVVNVVNVNNVNNVVVNNVVNVVNNNNNVNVNNVVVNNNVVVVNVVVVVNVNNNNVVVNNNNNVNVNVNNVVNVNNNVNVNVNVVNVVVNNNNNNNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVVVVVVVVNNNVNVNVVVNNVVVVVVVVVNVVVVNVNVNNVVNVVVVVNNNNNVVVVNNNVVVNNNVVNVNNVNNNVNNNVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVNVNVVVNVNNVVNVVNNNNNNVVNNNNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNVVNVVNVNVVNVNVVVVVVVNVNNNVVNNVVVNNNNVNVVNVVNVNNVVNVVVVNVVVNVNVVNVNNVNVVNVVNVNNNNNVVNNVNNNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVNVNNNNNVVNNNNVNNNNVVVVNNVVVNVNNVNVVVVVNNNNNNNVNNVNVNNVVVVNNNNNNVVVNVVVVVVVVNVNVVVNVNVNVVNVNNNVVNVNNNVNVNVNVVNVVNVVNNNNVNNVVNVVVNVVVVNVNVNNVNNNNVNNNNNVVVNNNNVVNNVNNVVNVVNVNNNNVNNNNVNVVVNVNNNVVNVVNNNVNNVNNNNNNVNNNVNVNVNVVVNVVNNVNNNVVNVVVVNVNVNNNVVVNNNNNNVVNVNNNNVVVVVVVNVVVVVNNVVVVVNNVVVNNNVVVVNVNNVNVNNNNNNVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNVNVNVNVVVNVVNVNNVVVVNNNNVNNNNVNVNNVNVNNNNVVVNVVNNNVNNVVVVVNVNNNVNNNNNVVVVVVVVVNVVVVVNNVNVVVNNNVVNVVNVVVNVVVVVNNNNNNNNNNNNNNVNNVNNNVNNNNNVNVVNVVNVNNNVVVNVVVNNVNNNVVVNVNNVVNNNNNNNNNNNNVVVNVVNVNNNVVVNVNVVVNVVVNVVVNNNNVVNNNNNVVVNVNNNNNNNVVVNNVNVNVNVNNNVVNVNNVVVNVVVVVNVNNVVNNVNVNVNVVVNVVVVNVVVNNVNNNNVNVVNVNVVNNVVNVNVNNVVVNVNNNVVNNNVNVVNNNNVVVNNVVVVVNNNNNNNVVVNVNVVVVNNVNNNNVVNVVVNNNNVVNNVNNNNNNNNVNNNVNNNNNNNNVNNVNVVVNVVNNVNNVVNNNNNNVNNVVNNVVVVNNVNNNVVVNVNNVVNVNVNNNVNNVNNVNNNNNVNNVNNVNNNNVVNVNNVNNVNVNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVNVNVNVVNNNVNNNNNNVNVNVNVVVNNVNNNVNVVVVNVNNNVNVNVNVVVNVVNVVVVVNNVVVNNNNVVNNVVNNNVNVNNNVNNVNNVNNNVNNNVNVVNVVNVVVNNNVNNNVNNNNNNNNNNNNNVVVNVVVNVVVVVVNVVNVNVVVNNNVNNNNVVNVNVVVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNVNNVNNNVVVNVVVNNNNNNNVNVNNNVNNVVNNNNNVNNNNNVNNVNVNNNNVVNNVVVVNVNVVVVNNVNVNVVVVVNNNVNNNNVVNVNNNNVNVVNNNNNNVNVNVVVNNVVNNNNVNNVNVVNNNNNVNNNVNVNNNNVNNVVNNVNNVNVNNVNNNVVVVNVNVNVNNVNVNVVVVVNVNVNNNVNVNVNNVNVNVNNVVNNNVVNNVVNVNVVNVVNNNNVNNNNNNVNVVVNNNVVVNVNVVVVVNVVVVNNNVVNVNNNNVVNVVNNVNVVVNVVVNNNVVNNVVVNNVVNNVVVVNNNNNVVVVNNNNNNVNVVVNVVNVNVVNNNVNNVNVNVVNNVVVNNVVVVVNNVNVVNNVVNVVVNVNVNVNVNNVVVVVVVVNNVVVVNVVVNVNNNNNNVVVNNNNVNNVVVNVVNVNNVVNNVNVNNVNVVVNNNVNVNVNVNNVVVVNNNNNVVVVNVVNVVVVNNNVVNVVNNVVVNNNVNNNNVNNNNVVNVNNVVVNNNNVNVVNVVNVNVNNVVVVNNVNNNVVVVVNVVNVVNVNNNNNVVNNVNNNVNNNNNVNVVVNVVVNVNVNNNVNVVVNNNVVVNNVVNVVNNVNVNNNNNVVVNNNVNVNNVNNVVNVVNVNNVVVVVVNNNVNNNVNNVVVVNVNVNVNNVVVNNNVNNNNNNNVVNNNNNNNNNVVVNVVNNNVNNVVNVNVVVVNVVVVNNVNVVVNNVVVVNNNVVVVNNVVVNVNVVVVNVVVVNNNVNVVNVNVVVNNVNNVVNVVVNNNNVNVVVVVVVVNNNVNNVNNNNVNVVNNVVVNNNNVVVVVNNVNNNNNVVVNVVNNVNVNNVVVVVVNVNVVVVNVNVNNNNNNNVVNVNVNVVNNVVNVNNVNVNNNNNNVVNNNVVVVVNNNNVVVVNNVVVVVVVVNNVNNVVNNVNVVNNNNVVNNVNNVVVVVNVVVNNVVVVNVNNVNNNVVVNVVVVVVVVNNVNVNVVNNVNVVVVVVNNVVVNVVVNVVVVNVVNNNVNNNVVVVNVVNVVNNVVVNVVVVNNVVNNVNNVNVNVNNVNVNNNNNVNNNVVVVNNNNNNVNNVNVNNVNVNNNNNNNVNNNNNVVVVVVVNNNVVVNNNVVNVVVNNNNNNVVVVVVNVVVVVNNVVVVNVVVNNVVNVVVVNNNNNNVVNVNNVVVNNNNVVNNVNNNVVVVVVNVVVNVNVNVNNVVVNNNVNVNNNVNNNNNNNVNNVVVNNVNNVNNNVNVNVNNNNNVVVVNVNVNVVVVVVVVVVVVVVVVVNNNVNNVVNVVNVVNVVVNNVNVVNNNNVVVVVVNNNNNVNNNNNVNNNVVNNNVNVVVNNVNNNVNNVVNNNNNVVVVVVVNVNVVVVNNVNNNNVNNNNNVNVVVVNVNNNVVVNNVVNNNVNNVVVNVNVVNNNVVNNNVVVVNVVNNVVNNVVNVVNVNVNNNNNNNVNNVNVVVVVNNVNVNNNNNVNVNVVVVVNVNVNNVNNVNNNVVVNNVVNVVVVVVNVNVVVVVVNVNVVVVNNVVVNVVVNVNNVVNNNVNVVNNNVVVNVNNNNVNVNVVVVVVVVVVNVVNNNNNNVVVNVVNNNVNVNNVNVNNVVNVVNNVVVVNVVVNNNVVVVVVNNNVNNVVVVNVNNNVNVVNVVNNNNVNVVNVNNVVVNNNVVNVVNNNNVVVVVVVNVVNNVVNNNNVVVVVVNNVVVVNNVNNVNVNNVNNVVNVVNNNNNVNVVNVNVVVNVNVNVVNVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVVNNNNVNNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVVVNNVVVNNNNNNVVNNNNVVVNNNNVVNNVNNVNVNVVVVNNNNVNVVNNNVNVNNVVNVVVNNNVVNNVNVVNNNVNVNVNVNVNVVVVVNNNNVVNVNNNVNNNVNNNNNNNVNVVNVNNNVNNNVNNNNVVVVNVVNVVNVNNNVNNNNNNNNNNVNNNVNVNVNVNVNVNNVNVNVNVVVNVVVVNNVNVNVNNNVNNNNNNVNVVVVNNVNNNNNNNNVNNVNNVVNNVNVVVVVNVVNNNVVNVVVVNVVNNNVNVNNVNVVVNVNVVVVVVVNNNNNNNNVNVNNNVNNNNNVNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNNNNVVVVVVNNVVVNVVNNVVVVNNNNVNVNVNNVVNVNNVVVNVVNVVNNVNVNNVN'
answer_open_question_9='...'
