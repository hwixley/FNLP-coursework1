lm_stats=
best10_ents=
worst10_ents=
answer_open_question_3=
answer_open_question_4=
mean=
std=
best10_ascci_ents=
worst10_ascci_ents=
best10_non_eng_ents=
worst10_non_eng_ents=
answer_open_question_6=
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[7.645645775585578e-05, 7.437719469104356e-05, 0.00012304032772425798, 7.39304665711192e-05, 7.394179345097604e-05, 7.572782710921218e-05, 7.589415018154868e-05]
naive_bayes_posterior=[{'V': 0.500516147965906, 'N': 0.49948385203409396}, {'V': 0.49983453212894, 'N': 0.5001654678710599}, {'V': 0.5432955462985688, 'N': 0.4567044537014312}, {'V': 0.5432955462985688, 'N': 0.4567044537014312}, {'V': 0.5148924156472404, 'N': 0.48510758435275964}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.749442931418668
answer_open_question_8='The best accuracy was achieved using a sequence of words. Indicating that\nthis model is most useful when passed a sequence of words.\n\nMy NB accuracy is worse than all LR scores in Table 1. I believe this\ndifference can mainly be attributed to the NB independence assumption\nas this infers probability distributions about features that are likely\nnot true. Thus a model that does not assume any distribution would be\nmore useful.'
lr_predictions=
answer_open_question_9=
