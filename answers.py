lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
best10_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056123, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.5686534278173125, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
worst10_ents=[(17.523736748003564, ['作品によっては怪人でありながらヒーロー', 'あるいはその逆', 'というシチュエーションも多々ありますが', 'そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね', 'あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です']), (17.524868750262904, ['ロンブーの淳さんはスピリチュアルスポット', 'セドナーで瞑想を実践してた', 'これらは偶然ではなく必然的に起こっている', '自然は全て絶好のタイミングで教えてくれている', 'そして今が今年最大の大改革時期だ']), (17.5264931699585, ['実物経済と金融との乖離を際限なく広げる', 'レバレッジが金融で儲けるコツだと', 'まるで正義のように叫ぶ連中が多いけど', 'これほど不健全な金融常識はないと思う', '連中は不健全と知りながら', '他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる']), (17.527615646393077, ['一応ワンセット揃えてみたんだけど', 'イマイチ効果を感じないのよね', 'それよりはオーラソーマとか', '肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい', '波動系ならバッチよりはホメオパシーの方がわかりやすい']), (17.53293217459052, ['慶喜ほどの人でさえこうなんだから', '並の人間だったらなおさら参謀無しじゃ何も出来ない', '一般に吹聴されてる慶喜のネガティブ論は', 'こうした敵対勢力による相次ぐテロに対して終始無関心で', '慶喜個人だけに批判を向けがち']), (17.541019489814225, ['昨日のセミナーではお目にかかれて光栄でした', '楽しく充実した時間をありがとうございました', '親しみのもてる分かりやすい講演に勇気を頂きました', '素晴らしいお仕事とともに益々のご活躍願っております', '今後ともよろしくお願いします']), (17.541411086467402, ['自民党が小沢やめろというなら', '当然町村やめろというブーメランがかえってくるわけです', 'おふたりとも選挙で選ばれた正当な国民の代表ですから', 'できればどちらにもやめてほしくありません', 'そろそろこんな不毛なことはやめにしてほしい']), (17.5427257173663, ['知識欲というのは不随意筋でできている', 'どうせ人間には永久に解明できないんだから', '宇宙はある時点で生まれたのか', 'それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ', '心臓に止まれと命令しても止まらないのと同じことだ']), (17.547644050965395, ['と言いつつもやっぱり笑えない時はあるよなあ', '笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの', '自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった', '今の自分に必要な経験だったとは思うけど', '出来ればあんな感情は二度とごめんだ']), (17.55280652132174, ['中身の羽毛は精製過程で殺菌処理しているから', '羽毛布団からダニが湧くことはない', 'あと羽毛布団の生地は糸の打ち込み本数が多く', '羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない', 'ただダニが布団に付着することはあるから手入れは必要'])]
answer_open_question_3='The entropy values represent the average uncertainty the model\nhas with classifying all the words in the given tweet.\nThe first tweets are all English words, the most common being\nconjunctions ("and"), noun articles ("the"), and nouns\n("weather", "love").\nThe last tweets mainly consisted of non-ASCII logograms from\nother languages. This was to be expected given these languages\nare evidently not likely to be used in an English tweet.'
answer_open_question_4='We should remove all non-English tweets (non-ASCII) from the corpus\nas these characters/words are obviously not relevant for\ndeveloping an English NL model.\nWe can identify non-English tweets by checking if they contain \nnon-ASCII characters as ASCII is only used for the English language.'
mean=3.8435755769050926
std=0.47772976561662
best10_ascci_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056123, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.5686534278173125, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
worst10_ascci_ents=[(5.166314124571327, ['hoje', 'nossa', 'amiga', 'espero', 'q', 'sorte', 'tenha', 'vc']), (5.166378486661663, ['aok', 'berlin', 'brandenburg', 'bürofläche', 'commercial', 'engel', 'immobilien', 'meldung', 'mietet', 'potsdam', 'potsdam', 'qm', 'v', 'völkers']), (5.166607294898407, ['mi', 'rt', 'ixxi', 'squeciduu', 'yinha']), (5.166636591109558, ['asdhiasdhiuadshiuads', 'rt', 'tentaando', 'to', 'x']), (5.166680391667252, ['aaaaaai', 'aqui', 'e', 'é', 'gente', 'guri', 'horror', 'lente', 'não', 'não', 'o', 'olho', 'que', 'que', 's', 'tem', 'tem', 'um', 'vermelho']), (5.166696700850563, ['aaaon', 'aah', 'as', 'cow', 'da', 'eu', 'lindas', 'parade', 'vaquinhas', 'viii']), (5.16672881298096, ['bra', 'di', 'douglas', 'douglas', 'ett', 'finansmannen', 'för', 'för', 'gustaf', 'gustaf', 'konkurrensen', 'och', 'skogsägaren', 'slag', 'slår', 'sveaskog']), (5.166820287702661, ['bola', 'macaé', 'rolando', 'vasco', 'x']), (5.1670133043123725, ['enaknya', 'hari', 'hmmm', 'ini', 'kmn', 'ya']), (5.16719955611981, ['ad', 'emg', 'ha', 'haha', 'ak', 'jg', 'k', 'kreta', 'krta', 'ksmg', 'mau', 'mba', 'naik', 'rt', 'smg', 'wkwk'])]
best10_non_eng_ents=[(4.321320405509358, ['afganistán', 'asociación', 'de', 'de', 'mujeres', 'rawa', 'revolucionarias']), (4.321322108677053, ['carrey', 'face', 'feat', 'mariah', 'minaj', 'my', 'nicki', 'out', 'rt', 'up', 'video', 'xxlmag', 'com']), (4.321338322311484, ['abisss', 'aja', 'demo', 'gini', 'hari', 'hikmah', 'mantabsss', 'membawa', 'sepi', 'sudirman', 'thamrin', 'tiap', 'trnyta']), (4.321374586541906, ['a', 'a', 'agora', 'com', 'consegui', 'd', 'de', 'dormir', 'dormir', 'durmo', 'e', 'eu', 'inteira', 'mas', 'nao', 'nao', 'nao', 'nao', 'noite', 'noite', 'nove', 'q', 'se', 'sono', 'to', 'vou']), (4.321385367081537, ['eh', 'meu', 'o', 'que', 'twitter', 'esse']), (4.321390995790845, ['don', 't', 'get', 'giggle', 'i', 'i', 'oh', 'oh']), (4.321500139376771, ['am', 'geburtstag', 'gefeiert', 'klingende', 'november', 'töne', 'wird']), (4.321525080235015, ['attraktion', 'belønning', 'fest', 'kærlighed', 'lykke', 'privacy', 'succes', 'tarot', 'tillid', 'udholdenhed']), (4.321575437028815, ['cerca', 'de', 'de', 'deci', 'del', 'exameeen', 'examen', 'le', 'mateeee', 'matematica', 'pueda', 'que', 'rt', 'shhuu', 'shuuu', 'sientese', 'valla', 'wayyy', 'y']), (4.321622845063304, ['abre', 'china', 'huaehuiaieh', 'o', 'olho'])]
worst10_non_eng_ents=[(5.166314124571327, ['hoje', 'nossa', 'amiga', 'espero', 'q', 'sorte', 'tenha', 'vc']), (5.166378486661663, ['aok', 'berlin', 'brandenburg', 'bürofläche', 'commercial', 'engel', 'immobilien', 'meldung', 'mietet', 'potsdam', 'potsdam', 'qm', 'v', 'völkers']), (5.166607294898407, ['mi', 'rt', 'ixxi', 'squeciduu', 'yinha']), (5.166636591109558, ['asdhiasdhiuadshiuads', 'rt', 'tentaando', 'to', 'x']), (5.166680391667252, ['aaaaaai', 'aqui', 'e', 'é', 'gente', 'guri', 'horror', 'lente', 'não', 'não', 'o', 'olho', 'que', 'que', 's', 'tem', 'tem', 'um', 'vermelho']), (5.166696700850563, ['aaaon', 'aah', 'as', 'cow', 'da', 'eu', 'lindas', 'parade', 'vaquinhas', 'viii']), (5.16672881298096, ['bra', 'di', 'douglas', 'douglas', 'ett', 'finansmannen', 'för', 'för', 'gustaf', 'gustaf', 'konkurrensen', 'och', 'skogsägaren', 'slag', 'slår', 'sveaskog']), (5.166820287702661, ['bola', 'macaé', 'rolando', 'vasco', 'x']), (5.1670133043123725, ['enaknya', 'hari', 'hmmm', 'ini', 'kmn', 'ya']), (5.16719955611981, ['ad', 'emg', 'ha', 'haha', 'ak', 'jg', 'k', 'kreta', 'krta', 'ksmg', 'mau', 'mba', 'naik', 'rt', 'smg', 'wkwk'])]
answer_open_question_6="This question is rather vague because...\n1. It does not detail the era of English to use (ie. 1500s-2000 vs. 21st century).\n2. It does not detail the dialect(s) of English (ie. British vs. American English).\n3. The source of English (ie. where we extract the data from). Corpora always have a genre\nthat denotes where the data was extracted from, this has a massive affect on the type of\nEnglish used (ie. News articles vs. Twitter data).\nThus I will assume we are referring to 21st century British English from a balanced Web corpus.\n\nExperiment:\n1. Get a 21st-century British English corpus with a balanced Web genre.\n2. Tokenise the corpus.\n3. Compute word frequencies for all the words in the corpus.\n4. Compute word priors by dividing the word frequency by the sum of all frequencies.\nThese priors should be smoothed to ensure we have no zero probabilities.\n5. Calculate entropy of each word using its prior.\n4. Take the mean of these entropies.\n\n\n1. Given Zipf's law we know that the frequency "
naive_bayes_vocab_size=13521
naive_bayes_prior={'N': 0.5223306571799433, 'V': 0.47766934282005674}
naive_bayes_likelihood=[0.0001330380393641162, 1.3515294883759858e-05, 0.00013365897031402744, 1.2250662767947985e-05, 0.00013643620128912556, 7.163413706196727e-05, 7.625530748937835e-05]
naive_bayes_posterior=[{'N': 0.42241481120292135, 'V': 0.5775851887970787}, {'N': 0.7796519554340516, 'V': 0.22034804456594828}, {'N': 0.23702499434491603, 'V': 0.762975005655084}, {'N': 0.23702499434491603, 'V': 0.762975005655084}, {'N': 0.014717981786951596, 'V': 0.9852820182130485}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.7846001485516216
answer_open_question_8='The best accuracy was achieved using a sequence of words. Indicating that\nthis model is most useful when passed a sequence of words.\n\nMy NB accuracy is worse than all LR scores in Table 1. I believe this\ndifference can mainly be attributed to the NB independence assumption\nas this infers probability distributions about features that are likely\nnot true. Thus a model that does not assume any distribution would be\nmore useful.'
lr_predictions='VVVVVVVVVNVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNNVNNNVNNNVNNVVVNNVVVNVNVVNVNVNVNNNVVVNVNNNVNVNVNNVNVVVNNNNNVNNNNVVVNVVNVNNVNNVNNVNVNNNVVNVNNVNNNNVNNNNVVNNNNVNNNNVNNNNNVNNNVVVVVVVNVNVNNNNVVVVNVVVVVVNVNNNVVNVNVNNVNVNVVVNVNVNNNNVNVNVNNVNNVNNVNNNNVVNNNNVNVNNNNVNVVNNNNVVNVNVNNVNVNVNVVNVNNVNNVVNNNVNNVVNNNNNVNVNNVVVNNNVVVVNVVVVVVVNNNNVNVNNNNNVNVNVNNVVNVNNNNNVNVNVVNVVVNNNVVNNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVNVVVVVVNNNVNVNVVNNNVVVVVVVVVNVVVVNVNVNNVVNVVVNVNNNNNNVNVNNNNNVNNNVVNVNNVNNNVNNNVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVVVNVVVNVNNVVNVVNNNNVNVNNNNNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNNVNVVNVNVVNVNVVVVVVVNVNNNVVNNVVVNNNNVNVVNVVNVNVVVNVVVVVVVVNVNVVNVNNVNVVNVVNVVNNNNVVNNVNVNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVVVNNNNNVVNNVNVNNNNVVNVNNVVNNVNVVNVVVNVNNNNNNNVNNVNVNNVVVVNNNVNNVVNNVVVVVVVVNVNVNVVVNVNVVNVNNNVVNVNNNVNVNVNVVNVVNVVNNNNVNNVVNVVVNVVVVNVNVVNVNNNNVNNNVNNNVNNNNVVNNNNNVVVVVNVNNNNVNNNNVNVVVNNNNNVVNVVNNNVNNVNNNNNNVNNNVNVVVNVVVNVVNNVNNNVVNVVVVNVNNNNNVVVVNNVNNVNNVNNNNVVVVVVVNVVVVVNVVVVVVNNVVVNNNVVVNNVNNVNNNNNNNNVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNVNVNVVVVVNVVNVVNVVVNNNNNVNNNNVNVNNVNNNNNNVVNNNVNNNNNNVVVVVNVNNNVNNNNNVVVVVNVVVNVVVNVNNVNVVVNVVVVNVVNVVVNVVVVVNNNNNNNNNNNNNNVNNVNNNVNNVNNVVVVNVVNVNNNNVVNVVNNNVNNNVNVNNNNVVNNNVNNNVNNNNVVVNVVNVNNNVVVNVNNVVNVVVNVNVNNNNVVNNNNNVVVNVNNNNNNNVVVNNVNVNVNVNNVVVNVNNVVVNVVNVVNVNNVVNNNNVVVNVVVNVVVVVVVVNNVNNNNVNVVNNNVVNNVVNVNVNNVVVNVNNNVVVNNVVVVNNNNVNVNNVVVVVNNNNNNVVVVNVNVVVVVNVNNNNVVNVVVNNNNVVNNVNNNNNVNNVNNNVNNNNVNVNVNNNNVVVNVVNNVNNVVNNNNNNVNNVVNNNVVVNNVNNNVVVNNNNVNNVNVNNNVNNVNNVNNVNNVNNVNNVNNNNVVNVNVVNNVNNNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVNVNVNVVVNNVNNNNNNVNVNVNNVVNNVNNNVNVVVVNVNNNVNVNVNNVVNVVNVVNVVNNVNVNNNNVVNNVVNNNVNVNNNVVNVNNVNVNVVNNVNVVNVVNVVVNNNVNNNVNNNNNNNNNNNNNVVVNVVVNVVVNVVNVVNVNVVVNNVVNNNNVVNVNVNVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNVNNVNNNVVVNVVVNNNNNVNNNNNVNVNNVVNNNNNVNNNNNVNNVNVNNNNNVNNNVVVNVNVNVVNNVNVNVVVVVNNNVNNNNVVNVNNNNVNVVNNNNNNVNVNVVVVNVVNNNNVNNVNVVNNNNNVVNNVNNNNNNVNNVVNNVNNNNVNNVNVNVVVVNVNVNVNVVVVNVVVVVNVNVNNNVNNNVVNVNVNVNNVVNNNVNNNVNNVNVNNVVNNVNVNNNNNNVNVVVNNNVVVNNNVVNVVNVVVNNVNVNNVNNNNVVNVVNNVNVVVNVVVNVVVVNNVVVNNVVNNVVVVNNNNNVNVVNNNNNNVNVVVNVVNVNVVNNNVNNVNVNVVNNVVVVNNNVVVNNVNVVNNVNNVVVNVNVNVNVNNVVVVVVVVNNVVNVNVNVNVNNNNNNVVVNNNNVNNVVNVVVNVVNVVNNVNVNNVNVVVNNNVNVNVNVNNVVVVNNNNVVVVVNVVNVVVNNNVVVNVVNNVVVNNNVNNNNVNNNNVVVVNNVVVNNNNVNVVNVVNVNVNNVVVVNNVNVNVVVVVNNNNVVNVNNNNNVVNNVNNNVNNNNNVNVVVNVVVNNVVNNNNNVVVNNNVNVNNVVNVVNNVNVNNNNNVVVVNNNNVNNVNNVVVVVNVNVVVVVVVNNNVNNNVNNVNVVNVNVNVNNVVVNNNVNNNNVNNVVNNVNVNNNNVVVNVVNVNVNNVVNVNVVNVNVVVVNNVNVVVNVVVVVNNNVVVVNNVVVNNNVVVNNVVVNNNNNVVVNVNVVVNNVNNVVNVVVNNNNVNVVVVVVVVVNNNNNVNNNNNNVVNNVVVNNNNVVVNVNNVNNNNNVVVNVVNNVNVNNVNVVVVNVNVVVVVVVVNVNNNNNVVNVNVNVVNNVVNVNNVNVNNNNNNVVNNNVVVVVNNNNNVVVNVVNVVVVVVNNVNNVVNNVNVVNNNNVNNNVNNVNVVNNVVVNNVVVVNVNNVNNNNVVNVVVVVVVVNNVNVNVVNNVNVVVVVVNNVVVNVVVNVVVVNVVVNNVNVNVVVVNNVVNNNNVVVNVVVNNNVVNNVNNVNVNVNNVNVNNNNNVNVNVVVVNNNNNNNNNNNVNNVNNNNNNNVNVVNNNNVNVVVNVNNNVVVNVNVVVVVNNNNNNNVVVVVVNVNVVVNVVVVVNNVVNNVVNNVVNNNNNNNVVNVNNVVNNNNNVVNNVNNVVVVVVVNVVVNVNVNVNVVVVNVNVNNNVNVNNNNNNVVVNVVVNNVNNVNNNVNVNNNNNNNVVVVNVNVNVVVVVVVVVVVNVVVVVNNNVNNVNNVVNVNNVVVNNVVVVNNNNVVVVVVNNNNNVNVNNNVNNNVVNNNVVVVVNNVNVNVVNNVNNNNNVVVVVVVNVNVVVVNVVNNVNVNNNNNVNVVVVNVNNNVVVNNVVNNNVNNVVVNVNVVNNNVVNNNVVVVNVVNVVVNNVVNVVNVNVNNVVNNNVNNVNNVVVVNNVNVVNNVNVNVNVVVVVNVNVNNVNNVNNVVVVNNVNNVVVVVVNVNVVVVVVNVNVVVVNNVVVNNVVNNNNVVNNNVNVVNNNVVVNVNNNNVNNNVVVVVVVVVVNVVNNNNNNVVVNVVNNNVNVNNNNNNNVVNVNNNNVVVNVVNNNNVVVNVVNNNVNNVVNVNNNNNVNVVNNVNNNNVNVVNNNNVVVNNNVVVVVNNVNVVNVVVVNVVNNVVNNNNVVVVVVNVVVVVNNVNVVNVVNNVNVVNVVNNNNVVNVVNVNVVVNVNNNVVNVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVNNNVNNVNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVNVNNVVVNNNNNNVVNNNNVVVNNNNVVNNVNNVNVNVVVVNNNNVNNVNNNNVVNNVVNVVVNNNVVNNVNVVNNNVNVNVNVNVNVVVVVNNNNVVNVNNNVNNNNNNNNNNNVNVVVVNNNVNNNNNNNNVVNVNVVNNVNVNNNVVVNNNNNNVNVNNNVNVNVNNNVNVNNVNVNVNVNVNVNVVNNVNVNNNNNVNNNNNNVNVVVVNNVNNNNNNNNVNNVNNNVNNNNVVVVVNVVNNNVVNVVVNNVVNVNVNVNNVNVVNNVNVVVVVVNNNNNNNNNVNNNNVVNNNNNNNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNVNNVVVNVVNNVVVNVVNNVVVVNNVNVNVNNNNVVNVNNVVVNVVNVVNVNNVNNVN'
answer_open_question_9='I first decided to include the unformatted features individually as this will allow our\nmodel to fit classes based on specific values for these features (like a unigram). This proved particularly\nuseful for prepositions such as "of".\nNext I wanted to create features that would represent combinations of these features I did this by taking\nall unique tuple permutations of these features (ie. (f1, f2)). This proved useful as it helped\nthe model identify common feature combinations.\nLastly, I wanted to manually create features to represent common suffixes/values for the features.\nI did this for the verb by separating the suffix (ie. "ing", "ed) of the verb with it\'s verb to get the tense,\nand verb stem.\nI did this for the nouns by checking if they were plural (ended in "s"), formed a question (contained "?"),\nor equated to common values (ie. "million" or "%").\nI did not need to do this fo the prepositions due to the existence of the tuple feature combinations.'
